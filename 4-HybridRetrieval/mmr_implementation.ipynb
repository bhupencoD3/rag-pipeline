{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4ca8f9",
   "metadata": {},
   "source": [
    "#### Implementation of Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2457995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24e6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f67311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_data.txt'}, page_content='LangChain is an open-source framework for building applications with large language models (LLMs), providing modular tools for tasks like integrating with external data sources, creating retrieval-augmented generation (RAG) pipelines, managing conversation memory, and connecting to various LLM'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content=\"memory, and connecting to various LLM providers. Retrieval-Augmented Generation (RAG) is a technique that enhances an LLM's knowledge by retrieving relevant information from an external knowledge base, such as documents or a vector database, and then providing that information as context to the LLM\"),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='providing that information as context to the LLM to generate a more accurate and contextually relevant response. In a typical LangChain RAG pipeline, documents are loaded, split into chunks, converted into vector embeddings, and stored in a vector database; when a user query is received, the system'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='when a user query is received, the system retrieves the most relevant chunks from the database and uses them along with the query to generate a grounded answer via the LLM. LangChain effectively serves as the orchestration layer for RAG, providing the components to load data, manage vector stores,'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='components to load data, manage vector stores, and chain the retrieval and generation steps together into a seamless workflow. What specific project or use case are you considering for a LangChain RAG pipeline, such as a Q&A system over your own documents or a more complex conversational'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='own documents or a more complex conversational agent?Vector stores are a foundational component for building sophisticated retrieval-augmented generation (RAG) applications, and LangChain serves as the framework that seamlessly integrates them into a complete system.A vector store is a specialized'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='a complete system.A vector store is a specialized database optimized for storing and searching high-dimensional vectors, which are numerical representations of text, images, or other data created by an embedding model. This differs significantly from traditional databases that rely on keyword or'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='traditional databases that rely on keyword or exact-match searches. Instead, vector stores use vector similarity search algorithms to find data points whose vector representations are semantically similar to a given query vector. This semantic capability is essential for RAG, as it enables the'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='is essential for RAG, as it enables the system to retrieve external information based on the meaning of a user\\'s query, rather than simply matching keywords. When a user asks a question, the vector store efficiently locates and returns the most relevant \"neighboring\" document chunks, providing the'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='\"neighboring\" document chunks, providing the factual context needed to generate an accurate and grounded response.LangChain acts as the orchestration layer that brings all the necessary components of a RAG pipeline together, providing a standard interface for working with a wide array of vector'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='interface for working with a wide array of vector stores. A typical LangChain workflow involves loading documents from various sources, splitting them into manageable chunks, and using an embedding model to convert these chunks into vectors. LangChain then provides methods to add these documents'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content=\"then provides methods to add these documents and their embeddings to the chosen vector store. When a user query is received, LangChain's retriever component queries the vector store, retrieving the most relevant document chunks. Finally, LangChain structures this retrieved context along with the\"),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='structures this retrieved context along with the original query into a prompt, which is then passed to an LLM for final answer generation. This modular design allows developers to easily swap different vector store implementations without rewriting their core application logic, making it simpler'),\n",
       " Document(metadata={'source': 'langchain_data.txt'}, page_content='their core application logic, making it simpler and more efficient to build and experiment with powerful, data-aware AI applications')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader('langchain_data.txt')\n",
    "raw_documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_documents)\n",
    "\n",
    "chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568ad749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhupen/Python_Learning/RAG/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model='all-miniLM-L6-v2')\n",
    "vector_store = FAISS.from_documents(chunks,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eadc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={'k':3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15e9725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7faee73f7cb0>, search_type='mmr', search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21cc1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "                                      \n",
    "                                      Context:\n",
    "                                      {context}\n",
    "                                      \n",
    "                                      Question:{input}\n",
    "                                      \"\"\")\n",
    "\n",
    "llm = init_chat_model('groq:gemma2-9b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d28cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "478d4292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7faee73f7cb0>, search_type='mmr', search_kwargs={'k': 3}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context provided.\\n\\n                                      Context:\\n                                      {context}\\n\\n                                      Question:{input}\\n                                      ')\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7faedc1a42f0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7faedc1a4ec0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      ",According to the text, an LLM retrieves context from a **vector database**. \n",
      "\n",
      "Here's the relevant excerpt:\n",
      "\n",
      "\"when a user query is received, the system structures this retrieved context along with the original query into a prompt, which is then passed to an LLM for final answer generation. This ... vector database...\" \n",
      "\n",
      "\n",
      "Let me know if you have any other questions! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = {'input':'LLM retrieve context from?'}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(f'Answer:\\n,{response['answer']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2055acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      ",According to the text, a vector is a **numerical representation of text, images, or other data created by an embedding model**.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = {\"input\":\"What is the term 'Vector'\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(f'Answer:\\n,{response['answer']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7b2165c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What is the term 'Vector'\",\n",
       " 'context': [Document(id='910525fb-f80c-4f90-a4fa-93fd6f052903', metadata={'source': 'langchain_data.txt'}, page_content='a complete system.A vector store is a specialized database optimized for storing and searching high-dimensional vectors, which are numerical representations of text, images, or other data created by an embedding model. This differs significantly from traditional databases that rely on keyword or'),\n",
       "  Document(id='724ab339-eae4-464e-b8cf-7aaddce2d432', metadata={'source': 'langchain_data.txt'}, page_content='\"neighboring\" document chunks, providing the factual context needed to generate an accurate and grounded response.LangChain acts as the orchestration layer that brings all the necessary components of a RAG pipeline together, providing a standard interface for working with a wide array of vector'),\n",
       "  Document(id='fec0e40f-008f-41d7-96aa-e1e78e615285', metadata={'source': 'langchain_data.txt'}, page_content='their core application logic, making it simpler and more efficient to build and experiment with powerful, data-aware AI applications')],\n",
       " 'answer': 'According to the text, a vector is a **numerical representation of text, images, or other data created by an embedding model**.  \\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8409d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
