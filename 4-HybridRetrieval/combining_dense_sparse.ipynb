{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7676b7a",
   "metadata": {},
   "source": [
    "#### Hybrid Retriever-Combining Dense And Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27bef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f5a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(page_content=\"Sparse retrieval methods like BM25 rely on keyword matching.\"),\n",
    "    Document(page_content=\"Dense retrieval uses embeddings to capture semantic similarity between texts.\"),\n",
    "    Document(page_content=\"Hybrid retrieval combines sparse and dense methods for better accuracy.\"),\n",
    "    Document(page_content=\"A vector database stores embeddings for fast similarity search.\"),\n",
    "    Document(page_content=\"ElasticSearch is a popular tool for keyword-based search.\"),\n",
    "    Document(page_content=\"Pinecone and Weaviate are commonly used for dense vector retrieval.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2037c06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhupen/Python_Learning/RAG/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model='all-miniLM-L6-v2')\n",
    "dense_vectorstore = FAISS.from_documents(docs,embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b942d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Sparse Retriever(BM25)\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3\n",
    "\n",
    "## combine with Ensemble Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever,sparse_retriever],\n",
    "    weights=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7703b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document 1:\n",
      "Sparse retrieval methods like BM25 rely on keyword matching.\n",
      "\n",
      " Document 2:\n",
      "Pinecone and Weaviate are commonly used for dense vector retrieval.\n",
      "\n",
      " Document 3:\n",
      "Hybrid retrieval combines sparse and dense methods for better accuracy.\n",
      "\n",
      " Document 4:\n",
      "Dense retrieval uses embeddings to capture semantic similarity between texts.\n",
      "\n",
      " Document 5:\n",
      "ElasticSearch is a popular tool for keyword-based search.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the Sparse retrieval methods?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "for i,doc in enumerate(results):\n",
    "    print(f\"\\n Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe25491",
   "metadata": {},
   "source": [
    "##### RAG pipeline with hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35baa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c482c65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fa6e0114c20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fa6e01156a0>, root_client=<openai.OpenAI object at 0x7fa6e1b62270>, root_async_client=<openai.AsyncOpenAI object at 0x7fa6e0115400>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "                                      \n",
    "                                      Context:\n",
    "                                      {context}\n",
    "                                      \n",
    "                                      Question:{input}\n",
    "                                      \"\"\")\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bed88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fa6fd93c590>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7fa6fd93d7f0>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\n                                      Context:\\n                                      {context}\\n\\n                                      Question:{input}\\n                                      ')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fa6e0114c20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fa6e01156a0>, root_client=<openai.OpenAI object at 0x7fa6e1b62270>, root_async_client=<openai.AsyncOpenAI object at 0x7fa6e0115400>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm=llm,prompt=prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever,combine_docs_chain=document_chain)\n",
    "\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4be0bc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: To create sparse retrieval, you can use methods such as BM25, which rely on keyword matching. Other techniques could include TF-IDF (Term Frequency-Inverse Document Frequency) or other boolean retrieval methods that focus on the presence or absence of specific keywords in the text.\n",
      "Source Documents:\n",
      "\n",
      "Doc 1: Hybrid retrieval combines sparse and dense methods for better accuracy.\n",
      "\n",
      "Doc 2: Sparse retrieval methods like BM25 rely on keyword matching.\n",
      "\n",
      "Doc 3: Dense retrieval uses embeddings to capture semantic similarity between texts.\n",
      "\n",
      "Doc 4: A vector database stores embeddings for fast similarity search.\n"
     ]
    }
   ],
   "source": [
    "query = {\"input\":\"What methods i can use to create sparse retrieval?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print('Answer:',response['answer'])\n",
    "\n",
    "print(f\"Source Documents:\")\n",
    "for i,doc in enumerate(response['context']):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9840a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The popular tool for keyword-based search is ElasticSearch.\n",
      "Source Documents:\n",
      "\n",
      "Doc 1: ElasticSearch is a popular tool for keyword-based search.\n",
      "\n",
      "Doc 2: Sparse retrieval methods like BM25 rely on keyword matching.\n",
      "\n",
      "Doc 3: Hybrid retrieval combines sparse and dense methods for better accuracy.\n",
      "\n",
      "Doc 4: Dense retrieval uses embeddings to capture semantic similarity between texts.\n",
      "\n",
      "Doc 5: A vector database stores embeddings for fast similarity search.\n",
      "\n",
      "Doc 6: Pinecone and Weaviate are commonly used for dense vector retrieval.\n"
     ]
    }
   ],
   "source": [
    "query = {\"input\":\"Popular tool for keyword-based search?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print('Answer:',response['answer'])\n",
    "\n",
    "print(f\"Source Documents:\")\n",
    "for i,doc in enumerate(response['context']):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e162bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
