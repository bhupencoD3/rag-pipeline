{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f21f06a",
   "metadata": {},
   "source": [
    "##### Load PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0bac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import(\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03904c",
   "metadata": {},
   "source": [
    "##### 1.PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af9e1c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages\n",
      "Page 1 content: Attention Is All Y ou Need\n",
      "Ashish V aswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer ∗\n",
      "Google B\n",
      "MetaDeta: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'subject': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'keywords': '12 Jun 2017, Neural Information Processing Systems, Machine translation, Encoder, BLEU, Speech translation, Google, University of Southern California', 'source': 'data/pdf/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pypdf_loader = PyPDFLoader(\"data/pdf/attention.pdf\")\n",
    "    pypdf_docs = pypdf_loader.load()\n",
    "    # print(pypdf_docs)\n",
    "    print(f\"Loaded {len(pypdf_docs)} pages\")\n",
    "    print(f\"Page 1 content: {pypdf_docs[0].page_content[:100]}\")\n",
    "    print(f\"MetaDeta: {pypdf_docs[0].metadata}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error\",e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbb35c",
   "metadata": {},
   "source": [
    "##### 2.PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a096e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages\n",
      "Page 1 content: Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brai\n",
      "MetaDeta: {'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': 'data/pdf/attention.pdf', 'file_path': 'data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'subject': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'keywords': '12 Jun 2017, Neural Information Processing Systems, Machine translation, Encoder, BLEU, Speech translation, Google, University of Southern California', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pymupdf_loader = PyMuPDFLoader(\"data/pdf/attention.pdf\")\n",
    "    pymupdf_docs = pymupdf_loader.load()\n",
    "    # print(pypdf_docs)\n",
    "    print(f\"Loaded {len(pymupdf_docs)} pages\")\n",
    "    print(f\"Page 1 content: {pymupdf_docs[0].page_content[:100]}\")\n",
    "    print(f\"MetaDeta: {pymupdf_docs[0].metadata}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error\",e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789be3d",
   "metadata": {},
   "source": [
    "###### Handling PDF challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94889d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "'Company Financial Report\\n\\n\\nthe finanacial performance for fiscal year 2025\\nshows significant growth '\n",
      "After:\n",
      "\"Company Financial Report the finanacial performance for fiscal year 2025 shows significant growth in probability. Revenue increased by 25% The company's efficiency improved due to workfiow optimization Page 1 of 10\"\n"
     ]
    }
   ],
   "source": [
    "# Example of raw PDF extraction\n",
    "raw_pdf_text = \"\"\"Company Financial Report\n",
    "\n",
    "\n",
    "the finanacial performance for fiscal year 2025\n",
    "shows significant growth in probability.\n",
    "\n",
    "Revenue increased by 25%\n",
    "\n",
    "The company's efficiency improved due to workflow\n",
    "optimization\n",
    "\n",
    "\n",
    "Page  1 of 10\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    text = text.replace(\"fl\",\"fi\")\n",
    "    text = text.replace(\"fL\",\"fl\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "cleaned_text = clean_text(raw_pdf_text)\n",
    "\n",
    "print(\"Before:\")\n",
    "print(repr(raw_pdf_text[:100]))\n",
    "print(\"After:\")\n",
    "print(repr(cleaned_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfae7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662b3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "class SmartPDFProcessor:\n",
    "    \"Advanced PDF processing with error handling\"\n",
    "    def __init__(self,chunk_size=1000,chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\" \"]\n",
    "        )\n",
    "    \n",
    "\n",
    "    def process_pdf(self,pdf_path:str) -> list[Document]:\n",
    "        # Load the pdf\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        processed_chunks = []\n",
    "\n",
    "        for page_num,page in enumerate(pages):\n",
    "            # clean the text\n",
    "            cleaned_text = self._clean_text(page.page_content)\n",
    "            \n",
    "            if len(cleaned_text.strip()) < 50:\n",
    "                continue\n",
    "\n",
    "            chunks = self.text_splitter.create_documents(\n",
    "                texts=[cleaned_text],\n",
    "                metadatas=[{\n",
    "                    **page.metadata,\n",
    "                    \"page\":page_num+1,\n",
    "                    \"total_pages\":len(pages),\n",
    "                    \"chunk_method\":\"smart_pdf_processor\",\n",
    "                    \"char_count\":len(cleaned_text)\n",
    "                }]\n",
    "            )\n",
    "\n",
    "            processed_chunks.extend(chunks)\n",
    "\n",
    "        return processed_chunks\n",
    "    \n",
    "    \n",
    "    def _clean_text(self,text:str) -> str:\n",
    "        \"Cleaned extracted text\"\n",
    "        text = \" \".join(text.split())\n",
    "        text = text.replace(\"fI\",\"fi\")\n",
    "        text = text.replace(\"fL\",\"fl\")\n",
    "\n",
    "        return text   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa4c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = SmartPDFProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fadafc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed into 40 smart chunks\n",
      "\n",
      "Sample chunk metadeta\n",
      "producer : PyPDF2\n",
      "creator : PyPDF\n",
      "creationdate : \n",
      "title : Attention is All you Need\n",
      "author : Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "subject : The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "keywords : 12 Jun 2017, Neural Information Processing Systems, Machine translation, Encoder, BLEU, Speech translation, Google, University of Southern California\n",
      "source : data/pdf/attention.pdf\n",
      "total_pages : 11\n",
      "page : 1\n",
      "page_label : 1\n",
      "chunk_method : smart_pdf_processor\n",
      "char_count : 2929\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    smart_chunks = preprocessor.process_pdf(\"data/pdf/attention.pdf\")\n",
    "    print(f\"Processed into {len(smart_chunks)} smart chunks\")\n",
    "\n",
    "    if smart_chunks:\n",
    "        print(\"\\nSample chunk metadeta\")\n",
    "        for key,value in smart_chunks[0].metadata.items():\n",
    "            print(f\"{key} : {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Processing Error:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d7b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
