{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb07847",
   "metadata": {},
   "source": [
    "##### Introduction to Data Ingetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4591989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Dict,Any\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8c29d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "print(\"Setup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b9fdb",
   "metadata": {},
   "source": [
    "###### Understanding Document Structure in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9d4b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure:-\n",
      "Content:This is the main text content that will be embedded and searched.\n",
      "metadata: {'source': 'example.txt', 'page': 1, 'author': 'bhopindrasingh parmar', 'date_created': '2025-08-23', 'custom_field': 'any_value'}\n"
     ]
    }
   ],
   "source": [
    "# create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"This is the main text content that will be embedded and searched.\",\n",
    "    metadata = {\n",
    "        \"source\":\"example.txt\",\n",
    "        \"page\":1,\n",
    "        \"author\":\"bhopindrasingh parmar\",\n",
    "        \"date_created\":\"2025-08-23\",\n",
    "        \"custom_field\":\"any_value\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Document Structure:-\")\n",
    "print(f\"Content:{doc.page_content}\")\n",
    "print(f\"metadata: {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7af41",
   "metadata": {},
   "source": [
    "###### Text File(.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d517ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fc98172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text file has created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"data/text_files/python_intro.txt\":'''\n",
    "Python is a high-level, interpreted programming language that has become one of the most popular tools in the tech world. Known for its simple syntax and readability, it allows developers to write clean and efficient code without getting lost in overly complex structures. Unlike low-level languages, Python abstracts many details of computer operations, making it beginner-friendly while still being powerful enough for advanced projects. Its vast ecosystem of libraries and frameworks gives it a huge advantage in almost every domain of computing.\n",
    "\n",
    "Python is used across a wide range of industries and applications. In web development, frameworks like Django and Flask help developers build robust websites and APIs. In data science and analytics, libraries like Pandas, NumPy, and Matplotlib are essential for handling and visualizing data. Python is also dominant in artificial intelligence and machine learning thanks to libraries such as TensorFlow, PyTorch, and Scikit-learn. Beyond that, it’s used in automation, scripting, cybersecurity, DevOps, and even game development. Its versatility is one of the main reasons it continues to dominate the programming landscape.\n",
    "\n",
    "Python in Machine Learning\n",
    "One of Python’s strongest suits is its role in machine learning (ML). Machine learning is a subset of artificial intelligence where systems learn patterns from data instead of being explicitly programmed. Python makes this process easier because of its extensive libraries and active community. Developers and researchers use it to build models that can predict outcomes, classify information, and even generate new content.\n",
    "\n",
    "Types of Machine Learning Techniques\n",
    "There are several types of machine learning techniques, each serving different purposes:\n",
    "\n",
    "Supervised Learning – This involves training a model on labeled data, meaning the inputs and correct outputs are known. It’s commonly used for tasks like spam detection, stock price prediction, or medical diagnosis.\n",
    "\n",
    "Unsupervised Learning – Here, the model works with unlabeled data to discover hidden patterns or groupings. A good example is customer segmentation, where businesses group users based on behavior without prior labels.\n",
    "\n",
    "Reinforcement Learning – In this technique, an agent learns by interacting with its environment and receiving rewards or penalties. It’s used in robotics, gaming (like AlphaGo), and self-driving cars.\n",
    "\n",
    "Semi-Supervised Learning – A mix of supervised and unsupervised approaches, where models are trained with a small portion of labeled data and a large portion of unlabeled data. It’s helpful when labeling data is expensive or time-consuming.\n",
    "\n",
    "Deep Learning – A subset of machine learning that uses neural networks with many layers to handle complex tasks like image recognition, natural language processing, and voice assistants.\n",
    "''',\n",
    "\n",
    "\"data/text_files/intro_to_rag.txt\":'''\n",
    "Retrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: information retrieval and text generation. Traditional large language models (LLMs) like GPT rely solely on their pre-trained knowledge, which is limited to what they’ve seen during training. This can cause them to produce outdated or incorrect information. RAG fixes this issue by retrieving relevant, up-to-date information from an external knowledge source—like a database, vector store, or even the web—and then using a language model to generate responses based on that retrieved context.\n",
    "\n",
    "How RAG Works\n",
    "The RAG process happens in two main steps: retrieval and generation. In the retrieval step, the system takes the user’s query and searches for the most relevant pieces of information, often using a vector database that stores text in numerical embeddings. Then in the generation step, the retrieved documents are passed to a large language model, which uses them as context to produce an accurate, coherent answer. This approach reduces hallucinations, improves factual accuracy, and ensures that the responses are grounded in real data rather than guesses.\n",
    "\n",
    "Applications of RAG\n",
    "RAG has a wide range of applications across industries. In customer support, it can provide instant, accurate answers by pulling information from product manuals or FAQs. In healthcare, it can assist doctors by retrieving the latest medical research before generating a summary. In legal and finance, it helps professionals by finding relevant case studies, policies, or market data to create well-informed reports. Essentially, RAG is the backbone of modern AI assistants that need to be both intelligent and reliable.\n",
    "\n",
    "Why RAG Matters\n",
    "The real strength of RAG lies in its ability to extend the capabilities of language models without retraining them from scratch. Instead of building an entirely new model every time knowledge changes, developers can simply update the external database. This makes RAG not only powerful but also scalable and cost-efficient. As AI continues to grow, RAG will play a crucial role in building applications that can reason over private, domain-specific, or constantly changing data sources.\n",
    "'''\n",
    "\n",
    "}\n",
    "\n",
    "for file_path,content in sample_texts.items():\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text file has created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca369460",
   "metadata": {},
   "source": [
    "TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d06dd4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document\n",
      "Content preview:\n",
      "Python is a high-level, interpreted programming language that has become one of the most popular to...\n",
      "metadata: {'source': 'data/text_files/python_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# loading a single text file\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\",encoding='utf-8')\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document\")\n",
    "print(f\"Content preview:{documents[0].page_content[:100]}...\")\n",
    "print(f\"metadata: {documents[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda41be",
   "metadata": {},
   "source": [
    "##### DirectoryLoader(reading entire dir[multiple files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7070fbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2871.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents\n",
      "\n",
      "Document 1\n",
      "Source:data/text_files/intro_to_rag.txt\n",
      "Length: 2210 characters\n",
      "\n",
      "Document 2\n",
      "Source:data/text_files/python_intro.txt\n",
      "Length: 2826 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "#load all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\", ## pattern to match files\n",
    "    loader_cls=TextLoader, ## loader class to use\n",
    "    loader_kwargs={'encoding':'utf-8'},\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "for i,doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}\")\n",
    "    print(f\"Source:{doc.metadata['source']}\")\n",
    "    print(f\"Length: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c74619",
   "metadata": {},
   "source": [
    "###### Text splitting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96ca3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import(\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f23cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/text_files/intro_to_rag.txt'}, page_content='\\nRetrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: information retrieval and text generation. Traditional large language models (LLMs) like GPT rely solely on their pre-trained knowledge, which is limited to what they’ve seen during training. This can cause them to produce outdated or incorrect information. RAG fixes this issue by retrieving relevant, up-to-date information from an external knowledge source—like a database, vector store, or even the web—and then using a language model to generate responses based on that retrieved context.\\n\\nHow RAG Works\\nThe RAG process happens in two main steps: retrieval and generation. In the retrieval step, the system takes the user’s query and searches for the most relevant pieces of information, often using a vector database that stores text in numerical embeddings. Then in the generation step, the retrieved documents are passed to a large language model, which uses them as context to produce an accurate, coherent answer. This approach reduces hallucinations, improves factual accuracy, and ensures that the responses are grounded in real data rather than guesses.\\n\\nApplications of RAG\\nRAG has a wide range of applications across industries. In customer support, it can provide instant, accurate answers by pulling information from product manuals or FAQs. In healthcare, it can assist doctors by retrieving the latest medical research before generating a summary. In legal and finance, it helps professionals by finding relevant case studies, policies, or market data to create well-informed reports. Essentially, RAG is the backbone of modern AI assistants that need to be both intelligent and reliable.\\n\\nWhy RAG Matters\\nThe real strength of RAG lies in its ability to extend the capabilities of language models without retraining them from scratch. Instead of building an entirely new model every time knowledge changes, developers can simply update the external database. This makes RAG not only powerful but also scalable and cost-efficient. As AI continues to grow, RAG will play a crucial role in building applications that can reason over private, domain-specific, or constantly changing data sources.\\n'), Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='\\nPython is a high-level, interpreted programming language that has become one of the most popular tools in the tech world. Known for its simple syntax and readability, it allows developers to write clean and efficient code without getting lost in overly complex structures. Unlike low-level languages, Python abstracts many details of computer operations, making it beginner-friendly while still being powerful enough for advanced projects. Its vast ecosystem of libraries and frameworks gives it a huge advantage in almost every domain of computing.\\n\\nPython is used across a wide range of industries and applications. In web development, frameworks like Django and Flask help developers build robust websites and APIs. In data science and analytics, libraries like Pandas, NumPy, and Matplotlib are essential for handling and visualizing data. Python is also dominant in artificial intelligence and machine learning thanks to libraries such as TensorFlow, PyTorch, and Scikit-learn. Beyond that, it’s used in automation, scripting, cybersecurity, DevOps, and even game development. Its versatility is one of the main reasons it continues to dominate the programming landscape.\\n\\nPython in Machine Learning\\nOne of Python’s strongest suits is its role in machine learning (ML). Machine learning is a subset of artificial intelligence where systems learn patterns from data instead of being explicitly programmed. Python makes this process easier because of its extensive libraries and active community. Developers and researchers use it to build models that can predict outcomes, classify information, and even generate new content.\\n\\nTypes of Machine Learning Techniques\\nThere are several types of machine learning techniques, each serving different purposes:\\n\\nSupervised Learning – This involves training a model on labeled data, meaning the inputs and correct outputs are known. It’s commonly used for tasks like spam detection, stock price prediction, or medical diagnosis.\\n\\nUnsupervised Learning – Here, the model works with unlabeled data to discover hidden patterns or groupings. A good example is customer segmentation, where businesses group users based on behavior without prior labels.\\n\\nReinforcement Learning – In this technique, an agent learns by interacting with its environment and receiving rewards or penalties. It’s used in robotics, gaming (like AlphaGo), and self-driving cars.\\n\\nSemi-Supervised Learning – A mix of supervised and unsupervised approaches, where models are trained with a small portion of labeled data and a large portion of unlabeled data. It’s helpful when labeling data is expensive or time-consuming.\\n\\nDeep Learning – A subset of machine learning that uses neural networks with many layers to handle complex tasks like image recognition, natural language processing, and voice assistants.\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78dcefe",
   "metadata": {},
   "source": [
    "###### method:1 character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61bc6e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRetrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: information retrieval and text generation. Traditional large language models (LLMs) like GPT rely solely on their pre-trained knowledge, which is limited to what they’ve seen during training. This can cause them to produce outdated or incorrect information. RAG fixes this issue by retrieving relevant, up-to-date information from an external knowledge source—like a database, vector store, or even the web—and then using a language model to generate responses based on that retrieved context.\\n\\nHow RAG Works\\nThe RAG process happens in two main steps: retrieval and generation. In the retrieval step, the system takes the user’s query and searches for the most relevant pieces of information, often using a vector database that stores text in numerical embeddings. Then in the generation step, the retrieved documents are passed to a large language model, which uses them as context to produce an accurate, coherent answer. This approach reduces hallucinations, improves factual accuracy, and ensures that the responses are grounded in real data rather than guesses.\\n\\nApplications of RAG\\nRAG has a wide range of applications across industries. In customer support, it can provide instant, accurate answers by pulling information from product manuals or FAQs. In healthcare, it can assist doctors by retrieving the latest medical research before generating a summary. In legal and finance, it helps professionals by finding relevant case studies, policies, or market data to create well-informed reports. Essentially, RAG is the backbone of modern AI assistants that need to be both intelligent and reliable.\\n\\nWhy RAG Matters\\nThe real strength of RAG lies in its ability to extend the capabilities of language models without retraining them from scratch. Instead of building an entirely new model every time knowledge changes, developers can simply update the external database. This makes RAG not only powerful but also scalable and cost-efficient. As AI continues to grow, RAG will play a crucial role in building applications that can reason over private, domain-specific, or constantly changing data sources.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = documents[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22128314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 590, which is longer than the specified 200\n",
      "Created a chunk of size 557, which is longer than the specified 200\n",
      "Created a chunk of size 519, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7 chunks\n",
      "First chunk: Retrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: inf...\n"
     ]
    }
   ],
   "source": [
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "char_chunk=char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunk)} chunks\")\n",
    "print(f\"First chunk: {char_chunk[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38508420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: information retrieval and text generation. Traditional large language models (LLMs) like GPT rely solely on their pre-trained knowledge, which is limited to what they’ve seen during training. This can cause them to produce outdated or incorrect information. RAG fixes this issue by retrieving relevant, up-to-date information from an external knowledge source—like a database, vector store, or even the web—and then using a language model to generate responses based on that retrieved context.\n",
      "------------------------------\n",
      "How RAG Works\n",
      "------------------------------\n",
      "The RAG process happens in two main steps: retrieval and generation. In the retrieval step, the system takes the user’s query and searches for the most relevant pieces of information, often using a vector database that stores text in numerical embeddings. Then in the generation step, the retrieved documents are passed to a large language model, which uses them as context to produce an accurate, coherent answer. This approach reduces hallucinations, improves factual accuracy, and ensures that the responses are grounded in real data rather than guesses.\n"
     ]
    }
   ],
   "source": [
    "print(char_chunk[0])\n",
    "print(\"-\"*30)\n",
    "print(char_chunk[1])\n",
    "print(\"-\"*30)\n",
    "print(char_chunk[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c365545",
   "metadata": {},
   "source": [
    "###### Method:2 Recursive Character splitting (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1a52f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36493532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 13 chunks\n",
      "First chunk: Retrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: inf...\n"
     ]
    }
   ],
   "source": [
    "recursive_chunks=recursive_splitter.split_text(text)\n",
    "print(f\"Created {len(recursive_chunks)} chunks\")\n",
    "print(f\"First chunk: {recursive_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd831baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: information retrieval and text generation. Traditional large language models (LLMs) like GPT rely\n",
      "------------------------------\n",
      "like GPT rely solely on their pre-trained knowledge, which is limited to what they’ve seen during training. This can cause them to produce outdated or incorrect information. RAG fixes this issue by\n",
      "------------------------------\n",
      "fixes this issue by retrieving relevant, up-to-date information from an external knowledge source—like a database, vector store, or even the web—and then using a language model to generate responses\n"
     ]
    }
   ],
   "source": [
    "print(char_chunk[0])\n",
    "print(\"-\"*30)\n",
    "print(char_chunk[1])\n",
    "print(\"-\"*30)\n",
    "print(char_chunk[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "734e99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text example - 10 chunks:\n",
      "\n",
      "chunk 1: 'Retrieval-Augmented Generation,or RAG, is a framework that combines two powerful'\n",
      "chunk 2: 'two powerful AI techniques: information retrieval and text'\n",
      "\n",
      "chunk 2: 'two powerful AI techniques: information retrieval and text'\n",
      "chunk 3: 'retrieval and text generation.Traditional large language models (LLMs) like GPT'\n",
      "\n",
      "chunk 3: 'retrieval and text generation.Traditional large language models (LLMs) like GPT'\n",
      "chunk 4: '(LLMs) like GPT rely solely on their pre-trained knowledge, which is limited to'\n",
      "\n",
      "chunk 4: '(LLMs) like GPT rely solely on their pre-trained knowledge, which is limited to'\n",
      "chunk 5: 'which is limited to what they’ve seen during training. This can cause them to'\n",
      "\n",
      "chunk 5: 'which is limited to what they’ve seen during training. This can cause them to'\n",
      "chunk 6: 'can cause them to produce outdated or incorrect information. RAG fixes this'\n",
      "\n",
      "chunk 6: 'can cause them to produce outdated or incorrect information. RAG fixes this'\n",
      "chunk 7: 'RAG fixes this issue by retrieving relevant, up-to-date information from an'\n",
      "\n",
      "chunk 7: 'RAG fixes this issue by retrieving relevant, up-to-date information from an'\n",
      "chunk 8: 'information from an external knowledge source—like a database, vector store, or'\n",
      "\n",
      "chunk 8: 'information from an external knowledge source—like a database, vector store, or'\n",
      "chunk 9: 'vector store, or even the web—and then using a language model to generate'\n",
      "\n",
      "chunk 9: 'vector store, or even the web—and then using a language model to generate'\n",
      "chunk 10: 'model to generate responses based on that retrieved context.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Created text without natural break points\n",
    "\n",
    "sample_text = \"Retrieval-Augmented Generation,or RAG, is a framework that combines two powerful AI techniques: information retrieval and text generation.Traditional large language models (LLMs) like GPT rely solely on their pre-trained knowledge, which is limited to what they’ve seen during training. This can cause them to produce outdated or incorrect information. RAG fixes this issue by retrieving relevant, up-to-date information from an external knowledge source—like a database, vector store, or even the web—and then using a language model to generate responses based on that retrieved context.\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],\n",
    "    chunk_size = 80,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "\n",
    "chunks = splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"\\nSample text example - {len(chunks)} chunks:\\n\")\n",
    "\n",
    "for i in range(len(chunks) - 1):\n",
    "    print(f\"chunk {i+1}: '{chunks[i]}'\")\n",
    "    print(f\"chunk {i+2}: '{chunks[i+1]}'\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4963dd8",
   "metadata": {},
   "source": [
    "##### method:3 Token-based splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4f9ecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 11 chunks\n",
      "First chunk:\n",
      "Retrieval-Augmented Generation, or RAG, is a framework that combines two powerful AI techniques: in...\n"
     ]
    }
   ],
   "source": [
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 10\n",
    ")\n",
    "\n",
    "token_chunk = token_splitter.split_text(text)\n",
    "print(f\"Total {len(token_chunk)} chunks\")\n",
    "print(f\"First chunk:{token_chunk[0][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
